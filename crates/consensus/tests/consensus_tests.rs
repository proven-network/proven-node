#[cfg(test)]
mod tests {

    use bytes::Bytes;
    use proven_consensus::{
        Consensus, ConsensusConfig, NodeId,
        global::{GlobalOperation, GlobalRequest},
    };

    use ed25519_dalek::SigningKey;
    use openraft::Config as RaftConfig;
    use proven_attestation_mock::MockAttestor;
    use proven_governance::{GovernanceNode, Version};
    use proven_governance_mock::MockGovernance;
    use rand::rngs::OsRng;
    use std::{collections::HashSet, sync::Arc, time::Duration};
    use tracing_test::traced_test;

    // Helper to create a simple single-node governance for testing
    fn create_test_governance(port: u16, signing_key: &SigningKey) -> Arc<MockGovernance> {
        let attestor = MockAttestor::new();
        let actual_pcrs = attestor.pcrs_sync();

        let test_version = Version {
            ne_pcr0: actual_pcrs.pcr0,
            ne_pcr1: actual_pcrs.pcr1,
            ne_pcr2: actual_pcrs.pcr2,
        };

        let topology_node = GovernanceNode {
            availability_zone: "test-az".to_string(),
            origin: format!("127.0.0.1:{port}"),
            public_key: signing_key.verifying_key(),
            region: "test-region".to_string(),
            specializations: HashSet::new(),
        };

        Arc::new(MockGovernance::new(
            vec![topology_node],
            vec![test_version],
            "http://localhost:3200".to_string(),
            vec![],
        ))
    }

    async fn create_test_consensus(port: u16) -> Consensus<MockGovernance, MockAttestor> {
        let signing_key = SigningKey::generate(&mut OsRng);
        let governance = create_test_governance(port, &signing_key);
        let attestor = Arc::new(MockAttestor::new());

        let config = ConsensusConfig {
            governance: governance.clone(),
            attestor: attestor.clone(),
            signing_key: signing_key.clone(),
            raft_config: RaftConfig::default(),
            transport_config: proven_consensus::transport::TransportConfig::Tcp {
                listen_addr: format!("127.0.0.1:{port}").parse().unwrap(),
            },
            storage_config: proven_consensus::config::StorageConfig::Memory,
            cluster_discovery_timeout: None,
            cluster_join_retry_config: proven_consensus::config::ClusterJoinRetryConfig::default(),
        };

        Consensus::new(config).await.unwrap()
    }

    fn next_port() -> u16 {
        proven_util::port_allocator::allocate_port()
    }

    #[tokio::test]
    #[traced_test]
    async fn test_consensus_creation() {
        let consensus = create_test_consensus(next_port()).await;

        // Verify basic properties
        assert!(!consensus.node_id().to_string().is_empty());

        println!("‚úÖ Consensus creation test passed");
    }

    #[tokio::test]
    #[traced_test]
    async fn test_consensus_lifecycle() {
        let consensus = create_test_consensus(next_port()).await;
        let node_id = consensus.node_id().to_string();

        println!("Testing consensus lifecycle for node: {}", &node_id[..8]);

        // Test start - this should now work since we fixed the compilation
        // Note: We're not calling start() since it's not implemented yet
        // but we can test basic functionality

        // Test that we can access the stream store
        let stream_store = consensus.stream_store();
        let last_seq = stream_store.last_sequence("test-stream").await;
        assert_eq!(last_seq, 0); // Should be 0 for a new stream

        let is_leader = consensus.is_leader();
        println!("Is leader: {}", is_leader);

        // Test shutdown
        let shutdown_result = consensus.shutdown().await;
        assert!(
            shutdown_result.is_ok(),
            "Consensus shutdown should succeed: {shutdown_result:?}"
        );

        println!(
            "‚úÖ Consensus lifecycle test passed for node: {}",
            &node_id[..8]
        );
    }

    #[tokio::test]
    #[traced_test]
    async fn test_consensus_messaging() {
        let consensus = create_test_consensus(next_port()).await;

        // Test cluster discovery (should work even without full cluster)
        let discovery_responses = consensus.discover_existing_clusters().await;
        assert!(
            discovery_responses.is_ok(),
            "Discovery should succeed: {discovery_responses:?}"
        );

        let responses = discovery_responses.unwrap();
        println!("Discovery responses: {}", responses.len());

        // Test stream store operations
        let stream_store = consensus.stream_store();

        // Test adding some data to the stream store directly
        let test_data = bytes::Bytes::from("test message");
        let result = stream_store
            .apply_operation(
                &GlobalOperation::PublishToStream {
                    stream: "test-stream".to_string(),
                    data: test_data.clone(),
                    metadata: None,
                },
                1,
            )
            .await;

        assert!(result.success, "Stream operation should succeed");
        assert_eq!(result.sequence, 1);

        // Test retrieving the message
        let retrieved = stream_store.get_message("test-stream", 1).await;
        assert_eq!(retrieved, Some(test_data));

        // Test last sequence
        let last_seq = stream_store.last_sequence("test-stream").await;
        assert_eq!(last_seq, 1);

        println!("‚úÖ Consensus messaging test passed");
    }

    #[tokio::test]
    #[traced_test]
    async fn test_leader_election_single_node() {
        let consensus = create_test_consensus(next_port()).await;
        let node_id = consensus.node_id().to_string();

        println!(
            "Testing single-node leader election for node: {}",
            &node_id[..8]
        );

        // Initially, the node should not be a leader (not initialized)
        assert!(!consensus.is_leader());
        // Raft always has a term (starts at 0), so we check it's 0 before initialization
        assert_eq!(consensus.current_term(), Some(0));
        assert_eq!(consensus.current_leader().await, None);

        // Start the consensus system - should automatically become leader (single node)
        let start_result = consensus.start().await;
        assert!(
            start_result.is_ok(),
            "Start should succeed: {start_result:?}"
        );

        // Wait for the cluster to be ready
        consensus
            .wait_for_leader(Some(Duration::from_secs(10)))
            .await
            .unwrap();

        // After initialization, this node should be the leader
        println!("Checking leadership status...");
        println!("Current term: {:?}", consensus.current_term());
        println!("Current leader: {:?}", consensus.current_leader().await);
        println!("Is leader: {}", consensus.is_leader());
        println!("Cluster size: {:?}", consensus.cluster_size());

        // In a single-node cluster, this node should have a term > 0 after initialization
        let current_term = consensus.current_term().unwrap_or(0);
        assert!(
            current_term > 0,
            "Term should be > 0 after initialization, got {}",
            current_term
        );
        assert_eq!(consensus.cluster_size(), Some(1));

        // The node should be the leader after waiting for cluster to be ready
        assert!(
            consensus.is_leader(),
            "Node should be leader after cluster is ready"
        );
        assert_eq!(consensus.current_leader().await, Some(node_id.clone()));
        println!("‚úÖ Node {} successfully became leader", &node_id[..8]);

        // Test that we can submit a proposal through Raft
        let request = GlobalRequest {
            operation: GlobalOperation::PublishToStream {
                stream: "leader-test".to_string(),
                data: Bytes::from("leader message"),
                metadata: None,
            },
        };

        // This will test if consensus is properly initialized and can accept proposals
        let proposal_result = consensus.submit_request(request).await;
        assert!(
            proposal_result.is_ok(),
            "Proposal should succeed after cluster is ready"
        );

        // Shutdown
        let shutdown_result = consensus.shutdown().await;
        assert!(
            shutdown_result.is_ok(),
            "Shutdown should succeed: {shutdown_result:?}"
        );

        println!("‚úÖ Single-node leader election test completed");
    }

    #[tokio::test]
    #[traced_test]
    async fn test_leader_election_workflow() {
        // Test that demonstrates leader election workflow works correctly
        let consensus = create_test_consensus(next_port()).await;
        let node_id = consensus.node_id().to_string();

        println!(
            "Testing leader election workflow for node: {}",
            &node_id[..8]
        );

        // Test 1: Node starts without being a leader
        assert!(!consensus.is_leader());
        assert_eq!(consensus.current_term(), Some(0));

        // Test 2: Start consensus (single node should become leader)
        let start_result = consensus.start().await;
        assert!(start_result.is_ok());

        // Wait for the cluster to be ready
        consensus
            .wait_for_leader(Some(Duration::from_secs(10)))
            .await
            .unwrap();

        // Test 3: Verify leadership is established
        let final_term = consensus.current_term().unwrap_or(0);
        assert!(
            final_term > 0,
            "Term should advance after leadership election"
        );

        let leader_id = consensus.current_leader().await;
        assert_eq!(
            leader_id,
            Some(node_id.clone()),
            "Node should be the leader"
        );

        assert!(consensus.is_leader(), "Node should report itself as leader");

        // Test 4: Test that leader can process requests
        let request = GlobalRequest {
            operation: GlobalOperation::PublishToStream {
                stream: "leadership-test".to_string(),
                data: Bytes::from("leader can process this"),
                metadata: None,
            },
        };

        let result = consensus.submit_request(request).await;
        assert!(result.is_ok(), "Leader should be able to process requests");

        // Test 5: Verify state machine received the request
        tokio::time::sleep(Duration::from_millis(100)).await;
        let stream_store = consensus.stream_store();
        let last_seq = stream_store.last_sequence("leadership-test").await;
        assert!(
            last_seq > 0,
            "Request should have been processed by state machine"
        );

        // Shutdown
        let shutdown_result = consensus.shutdown().await;
        assert!(shutdown_result.is_ok());

        println!("‚úÖ Leader election workflow test completed successfully");
        println!("   - Single node correctly became leader");
        println!("   - Term advanced from 0 to {}", final_term);
        println!("   - Leader processed requests successfully");
        println!("   - State machine applied the requests");
    }

    #[tokio::test]
    #[traced_test]
    async fn test_multi_node_cluster_formation() {
        println!("üß™ Testing multi-node cluster formation with shared governance");

        let num_nodes = 3;
        let mut nodes = Vec::new();
        let mut ports = Vec::new();
        let mut signing_keys = Vec::new();

        // Allocate ports and generate keys for all nodes
        for _i in 0..num_nodes {
            ports.push(next_port());
            signing_keys.push(SigningKey::generate(&mut OsRng));
        }

        println!("üìã Allocated ports: {:?}", ports);

        // Create a shared governance that knows about all nodes
        let shared_governance = {
            let attestor = MockAttestor::new();
            let actual_pcrs = attestor.pcrs_sync();
            let test_version = Version {
                ne_pcr0: actual_pcrs.pcr0,
                ne_pcr1: actual_pcrs.pcr1,
                ne_pcr2: actual_pcrs.pcr2,
            };

            let governance = Arc::new(MockGovernance::new(
                vec![], // Start with empty topology
                vec![test_version],
                "http://localhost:3200".to_string(),
                vec![],
            ));

            // Add all nodes to the shared governance
            for (i, (&port, signing_key)) in ports.iter().zip(signing_keys.iter()).enumerate() {
                let node = GovernanceNode {
                    availability_zone: "test-az".to_string(),
                    origin: format!("http://127.0.0.1:{}", port),
                    public_key: signing_key.verifying_key(),
                    region: "test-region".to_string(),
                    specializations: HashSet::new(),
                };

                governance
                    .add_node(node)
                    .expect("Failed to add node to governance");
                println!("‚úÖ Added node {} to shared governance", i);
            }

            governance
        };

        // Create consensus nodes using the shared governance
        for (i, (&port, signing_key)) in ports.iter().zip(signing_keys.iter()).enumerate() {
            let attestor = Arc::new(MockAttestor::new());

            let config = ConsensusConfig {
                governance: shared_governance.clone(),
                attestor: attestor.clone(),
                signing_key: signing_key.clone(),
                raft_config: RaftConfig::default(),
                transport_config: proven_consensus::transport::TransportConfig::Tcp {
                    listen_addr: format!("127.0.0.1:{port}").parse().unwrap(),
                },
                storage_config: proven_consensus::config::StorageConfig::Memory,
                cluster_discovery_timeout: None,
                cluster_join_retry_config:
                    proven_consensus::config::ClusterJoinRetryConfig::default(),
            };

            let consensus = Consensus::new(config).await.unwrap();
            nodes.push(consensus);
            println!(
                "‚úÖ Created node {} listening on port {} with shared governance",
                i, port
            );
        }

        // Start all nodes simultaneously to allow proper cluster formation
        println!("üöÄ Starting all nodes to enable cluster formation...");

        // Start all nodes sequentially but quickly - they all need to be listening before discovery
        for (i, node) in nodes.iter().enumerate() {
            println!("Starting node {} ({})", i, &node.node_id());
            let start_result = node.start().await;
            assert!(
                start_result.is_ok(),
                "Node {} start should succeed: {start_result:?}",
                i
            );
            // Very short delay to allow listener to start
            tokio::time::sleep(Duration::from_millis(100)).await;
        }

        println!("‚úÖ All nodes started and listening - networks ready for discovery");

        // Give extra time for all listeners to be fully ready and discovery to happen
        tokio::time::sleep(Duration::from_secs(2)).await;

        // Give time for cluster formation and leader election
        println!("‚è≥ Waiting for cluster formation and leader election...");
        tokio::time::sleep(Duration::from_secs(8)).await;

        // Collect cluster state from all nodes
        let mut leader_count = 0;
        let mut cluster_leaders = Vec::new();
        let mut cluster_sizes = Vec::new();
        let mut cluster_terms = Vec::new();

        println!("üìä Analyzing cluster state:");
        for (i, node) in nodes.iter().enumerate() {
            let is_leader = node.is_leader();
            let current_leader = node.current_leader().await;
            let cluster_size = node.cluster_size();
            let current_term = node.current_term();

            println!(
                "   Node {} - Leader: {}, Current Leader: {:?}, Term: {:?}, Cluster Size: {:?}",
                i,
                is_leader,
                current_leader.as_ref().map(|id| &id[..8]),
                current_term,
                cluster_size
            );

            if is_leader {
                leader_count += 1;
            }

            cluster_leaders.push(current_leader);
            cluster_sizes.push(cluster_size);
            cluster_terms.push(current_term);
        }

        // Assert correct cluster formation
        println!("üîç Verifying correct cluster formation...");

        // 1. There should be exactly 1 leader across all nodes
        assert_eq!(
            leader_count, 1,
            "Expected exactly 1 leader, found {}",
            leader_count
        );
        println!("‚úÖ Exactly 1 leader found");

        // 2. All nodes should report the same cluster size of 3
        let expected_cluster_size = Some(num_nodes);
        for (i, &cluster_size) in cluster_sizes.iter().enumerate() {
            assert_eq!(
                cluster_size, expected_cluster_size,
                "Node {} reports cluster size {:?}, expected {:?}",
                i, cluster_size, expected_cluster_size
            );
        }
        println!("‚úÖ All nodes report cluster size of {}", num_nodes);

        // 3. All nodes should agree on who the leader is
        let first_leader = &cluster_leaders[0];
        assert!(first_leader.is_some(), "No leader found");
        for (i, leader) in cluster_leaders.iter().enumerate() {
            assert_eq!(
                leader,
                first_leader,
                "Node {} reports leader {:?}, expected {:?}",
                i,
                leader.as_ref().map(|s| &s[..8]),
                first_leader.as_ref().map(|s| &s[..8])
            );
        }
        println!(
            "‚úÖ All nodes agree on leader: {}",
            &first_leader.as_ref().unwrap()[..8]
        );

        // 4. All nodes should have the same term (indicating same cluster)
        let first_term = cluster_terms[0];
        assert!(
            first_term.is_some() && first_term.unwrap() > 0,
            "Invalid term"
        );
        for (i, &term) in cluster_terms.iter().enumerate() {
            assert_eq!(
                term, first_term,
                "Node {} reports term {:?}, expected {:?}",
                i, term, first_term
            );
        }
        println!("‚úÖ All nodes agree on term: {}", first_term.unwrap());

        println!(
            "üéâ SUCCESS: Proper single-cluster formation with {} members and 1 leader!",
            num_nodes
        );

        // Test cluster functionality - only the leader should accept writes
        println!("üîç Testing cluster functionality...");
        let leader_node = nodes
            .iter()
            .find(|node| node.is_leader())
            .expect("Should have exactly one leader");

        let request = GlobalRequest {
            operation: GlobalOperation::PublishToStream {
                stream: "cluster-test".to_string(),
                data: Bytes::from("test message from cluster"),
                metadata: None,
            },
        };

        match leader_node.submit_request(request).await {
            Ok(_) => {
                println!("‚úÖ Leader successfully processed operation");
            }
            Err(e) => {
                panic!("Leader failed to process operation: {}", e);
            }
        }

        println!("‚úÖ Cluster functionality verified");

        // Graceful shutdown
        println!("üõë Shutting down all nodes...");
        for (i, node) in nodes.iter().enumerate() {
            let shutdown_result = node.shutdown().await;
            assert!(
                shutdown_result.is_ok(),
                "Node {} shutdown should succeed",
                i
            );
            println!("‚úÖ Node {} shutdown successfully", i);
        }

        println!("üéØ Multi-node cluster formation test completed successfully!");
        println!(
            "üéâ Verified: Single cluster with {} members and 1 leader",
            num_nodes
        );
    }

    #[tokio::test]
    #[traced_test]
    async fn test_simultaneous_node_discovery() {
        println!("üß™ Testing simultaneous node discovery");

        let num_nodes = 2; // Start with just 2 nodes for simplicity
        let mut nodes = Vec::new();
        let mut ports = Vec::new();
        let mut signing_keys = Vec::new();

        // Allocate ports and generate keys for all nodes
        for _i in 0..num_nodes {
            ports.push(next_port());
            signing_keys.push(SigningKey::generate(&mut OsRng));
        }

        println!("üìã Allocated ports: {:?}", ports);

        // Create a shared governance that knows about all nodes
        let shared_governance = {
            let attestor = MockAttestor::new();
            let actual_pcrs = attestor.pcrs_sync();
            let test_version = Version {
                ne_pcr0: actual_pcrs.pcr0,
                ne_pcr1: actual_pcrs.pcr1,
                ne_pcr2: actual_pcrs.pcr2,
            };

            let governance = Arc::new(MockGovernance::new(
                vec![], // Start with empty topology
                vec![test_version],
                "http://localhost:3200".to_string(),
                vec![],
            ));

            // Add all nodes to the shared governance
            for (i, (&port, signing_key)) in ports.iter().zip(signing_keys.iter()).enumerate() {
                let node = GovernanceNode {
                    availability_zone: "test-az".to_string(),
                    origin: format!("http://127.0.0.1:{}", port),
                    public_key: signing_key.verifying_key(),
                    region: "test-region".to_string(),
                    specializations: HashSet::new(),
                };

                governance
                    .add_node(node)
                    .expect("Failed to add node to governance");
                println!("‚úÖ Added node {} to shared governance", i);
            }

            governance
        };

        // Create consensus nodes using the shared governance
        for (i, (&port, signing_key)) in ports.iter().zip(signing_keys.iter()).enumerate() {
            let attestor = Arc::new(MockAttestor::new());

            let config = ConsensusConfig {
                governance: shared_governance.clone(),
                attestor: attestor.clone(),
                signing_key: signing_key.clone(),
                raft_config: RaftConfig::default(),
                transport_config: proven_consensus::transport::TransportConfig::Tcp {
                    listen_addr: format!("127.0.0.1:{port}").parse().unwrap(),
                },
                storage_config: proven_consensus::config::StorageConfig::Memory,
                cluster_discovery_timeout: None,
                cluster_join_retry_config:
                    proven_consensus::config::ClusterJoinRetryConfig::default(),
            };

            let consensus = Consensus::new(config).await.unwrap();
            nodes.push(consensus);
            println!(
                "‚úÖ Created node {} listening on port {} with shared governance",
                i, port
            );
        }

        // Start ALL nodes simultaneously
        println!("üöÄ Starting all nodes simultaneously...");
        let start_futures: Vec<_> = nodes.iter().map(|node| node.start()).collect();

        // Use try_join_all to start all nodes at the same time
        let start_results = futures::future::try_join_all(start_futures).await;
        assert!(start_results.is_ok(), "All nodes should start successfully");

        println!("‚úÖ All nodes started simultaneously");

        // Give time for cluster discovery
        println!("‚è≥ Waiting for cluster discovery...");
        tokio::time::sleep(Duration::from_secs(10)).await; // Give plenty of time

        // Check if any discovery responses were received
        println!("üìä Checking discovery results:");
        for (i, node) in nodes.iter().enumerate() {
            let cluster_state = node.cluster_state().await;
            let is_leader = node.is_leader();
            let current_leader = node.current_leader().await;

            println!(
                "   Node {} - State: {:?}, Leader: {}, Current Leader: {:?}",
                i,
                cluster_state,
                is_leader,
                current_leader.as_ref().map(|id| &id[..8])
            );
        }

        // Graceful shutdown
        println!("üõë Shutting down all nodes...");
        for (i, node) in nodes.iter().enumerate() {
            let shutdown_result = node.shutdown().await;
            assert!(
                shutdown_result.is_ok(),
                "Node {} shutdown should succeed",
                i
            );
            println!("‚úÖ Node {} shutdown successfully", i);
        }

        println!("üéØ Simultaneous discovery test completed");
    }

    // Helper function to create shared governance for multiple nodes
    #[allow(dead_code)]
    fn create_shared_governance(ports: &[u16]) -> Arc<MockGovernance> {
        let attestor = MockAttestor::new();
        let actual_pcrs = attestor.pcrs_sync();
        let test_version = Version {
            ne_pcr0: actual_pcrs.pcr0,
            ne_pcr1: actual_pcrs.pcr1,
            ne_pcr2: actual_pcrs.pcr2,
        };

        let mut topology_nodes = Vec::new();
        for port in ports {
            let signing_key = SigningKey::generate(&mut OsRng);
            let topology_node = GovernanceNode {
                availability_zone: "test-az".to_string(),
                origin: format!("http://127.0.0.1:{}", port),
                public_key: signing_key.verifying_key(),
                region: "test-region".to_string(),
                specializations: HashSet::new(),
            };
            topology_nodes.push(topology_node);
        }

        Arc::new(MockGovernance::new(
            topology_nodes,
            vec![test_version],
            "http://localhost:3200".to_string(),
            vec![],
        ))
    }

    #[tokio::test]
    #[traced_test]
    async fn test_consensus_transport_types() {
        // Test TCP transport creation
        let tcp_consensus = create_test_consensus(next_port()).await;
        assert!(!tcp_consensus.supports_http_integration());

        // Test WebSocket transport creation
        let signing_key = SigningKey::generate(&mut OsRng);
        let governance = create_test_governance(next_port(), &signing_key);
        let attestor = Arc::new(MockAttestor::new());

        let temp_dir = tempfile::tempdir().expect("Failed to create temp directory");

        let config = ConsensusConfig {
            governance: governance.clone(),
            attestor: attestor.clone(),
            signing_key: signing_key.clone(),
            raft_config: RaftConfig::default(),
            transport_config: proven_consensus::transport::TransportConfig::WebSocket,
            storage_config: proven_consensus::config::StorageConfig::RocksDB {
                path: temp_dir.path().to_path_buf(),
            },
            cluster_discovery_timeout: None,
            cluster_join_retry_config: proven_consensus::config::ClusterJoinRetryConfig::default(),
        };

        let ws_consensus = Consensus::new(config).await.unwrap();

        assert!(ws_consensus.supports_http_integration());

        println!("‚úÖ Transport types test passed");
    }

    #[tokio::test]
    #[traced_test]
    async fn test_stream_operations() {
        let consensus = create_test_consensus(next_port()).await;

        consensus.start().await.unwrap();

        // Wait for the cluster to be ready
        consensus
            .wait_for_leader(Some(Duration::from_secs(10)))
            .await
            .unwrap();

        // Test that we can access stream store directly
        let stream_store = consensus.stream_store();
        let last_seq = stream_store.last_sequence("test-stream").await;
        assert_eq!(last_seq, 0); // Should be 0 for a new stream

        // Test get_message (should return None for non-existent message)
        let result = consensus.get_message("test-stream", 1).await;
        assert!(result.is_ok());
        assert_eq!(result.unwrap(), None);

        // Test last_sequence
        let result = consensus.last_sequence("test-stream").await;
        assert!(result.is_ok());
        assert_eq!(result.unwrap(), 0);

        // Test route_subject (should return empty set for non-existent subject)
        let routes = consensus.route_subject("test.subject").await;
        assert!(routes.is_empty());

        // Test placeholder methods (these return None/empty until fully implemented)
        let stream_subjects = consensus.get_stream_subjects("test-stream").await;
        assert_eq!(stream_subjects, None);

        let subject_streams = consensus.get_subject_streams("test.*").await;
        assert_eq!(subject_streams, None);

        let all_subscriptions = consensus.get_all_subscriptions().await;
        assert!(all_subscriptions.is_empty());

        // Test leader status (should be true after cluster is ready)
        let is_leader = consensus.is_leader();
        assert!(is_leader, "Node should be leader after cluster is ready");

        // Test metrics (should be available after cluster is ready)
        let metrics = consensus.metrics();
        assert!(
            metrics.is_some(),
            "Metrics should be available after cluster is ready"
        );

        println!("‚úÖ Stream operations interface test passed");
    }

    #[tokio::test]
    #[traced_test]
    async fn test_unidirectional_connection_basic() {
        println!("üß™ Testing basic unidirectional connection message sending");

        let num_nodes = 2;
        let mut nodes = Vec::new();
        let mut ports = Vec::new();
        let mut signing_keys = Vec::new();

        // Allocate ports and generate keys for all nodes
        for _i in 0..num_nodes {
            ports.push(next_port());
            signing_keys.push(SigningKey::generate(&mut OsRng));
        }

        println!("üìã Allocated ports: {:?}", ports);

        // Create a shared governance that knows about all nodes
        let shared_governance = {
            let attestor = MockAttestor::new();
            let actual_pcrs = attestor.pcrs_sync();
            let test_version = Version {
                ne_pcr0: actual_pcrs.pcr0,
                ne_pcr1: actual_pcrs.pcr1,
                ne_pcr2: actual_pcrs.pcr2,
            };

            let governance = Arc::new(MockGovernance::new(
                vec![], // Start with empty topology
                vec![test_version],
                "http://localhost:3200".to_string(),
                vec![],
            ));

            // Add all nodes to the shared governance
            for (i, (&port, signing_key)) in ports.iter().zip(signing_keys.iter()).enumerate() {
                let node = GovernanceNode {
                    availability_zone: "test-az".to_string(),
                    origin: format!("http://127.0.0.1:{}", port),
                    public_key: signing_key.verifying_key(),
                    region: "test-region".to_string(),
                    specializations: HashSet::new(),
                };

                governance
                    .add_node(node)
                    .expect("Failed to add node to governance");
                println!("‚úÖ Added node {} to shared governance", i);
            }

            governance
        };

        // Create consensus nodes using the shared governance
        // Note: The consensus constructor automatically starts the network
        for (i, (&port, signing_key)) in ports.iter().zip(signing_keys.iter()).enumerate() {
            let attestor = Arc::new(MockAttestor::new());

            let config = ConsensusConfig {
                governance: shared_governance.clone(),
                attestor: attestor.clone(),
                signing_key: signing_key.clone(),
                raft_config: RaftConfig::default(),
                transport_config: proven_consensus::transport::TransportConfig::Tcp {
                    listen_addr: format!("127.0.0.1:{port}").parse().unwrap(),
                },
                storage_config: proven_consensus::config::StorageConfig::Memory,
                cluster_discovery_timeout: None,
                cluster_join_retry_config:
                    proven_consensus::config::ClusterJoinRetryConfig::default(),
            };

            let consensus = Consensus::new(config).await.unwrap();
            nodes.push(consensus);
            println!(
                "‚úÖ Created node {} listening on port {} with shared governance",
                i, port
            );
        }

        println!(
            "‚úÖ Created {} nodes (networks already started)",
            nodes.len()
        );

        // Wait a moment for listeners to be ready
        tokio::time::sleep(Duration::from_millis(200)).await;

        // Test sending a message from node 0 to node 1 (this will establish connection on-demand)
        let node_1_id = NodeId::new(signing_keys[1].verifying_key());
        let test_message = proven_consensus::network::messages::Message::Application(Box::new(
            proven_consensus::network::messages::ApplicationMessage::ClusterDiscovery(
                proven_consensus::network::messages::ClusterDiscoveryRequest {
                    requester_id: nodes[0].node_id().clone(),
                },
            ),
        ));

        // Send message which should establish connection on-demand
        let send_result = nodes[0]
            .network_manager()
            .send_message(node_1_id.clone(), test_message)
            .await;
        println!("üì§ Message send result: {:?}", send_result);

        // Wait a moment for connection to be established
        tokio::time::sleep(Duration::from_millis(300)).await;

        // Check connected peers
        let connected_peers = nodes[0]
            .network_manager()
            .get_connected_peers()
            .await
            .unwrap();
        println!("üì° Node 0 connected peers: {:?}", connected_peers);

        // Verify on-demand connection was established
        assert!(
            !connected_peers.is_empty(),
            "Node 0 should have connected peers after sending message"
        );
        assert_eq!(
            connected_peers.len(),
            1,
            "Node 0 should have exactly 1 connected peer"
        );

        println!("‚úÖ Basic unidirectional connection test completed successfully");
        println!("‚úÖ Confirmed on-demand connections work without block_on issues");

        // Cleanup
        for node in nodes {
            let _ = node.shutdown().await;
        }
    }

    #[tokio::test]
    #[traced_test]
    async fn test_cluster_discovery_with_correlation_ids() {
        println!("üß™ Testing cluster discovery with correlation ID tracking");

        let num_nodes = 2;
        let mut nodes = Vec::new();
        let mut ports = Vec::new();
        let mut signing_keys = Vec::new();

        // Allocate ports and generate keys for all nodes
        for _i in 0..num_nodes {
            ports.push(next_port());
            signing_keys.push(SigningKey::generate(&mut OsRng));
        }

        println!("üìã Allocated ports: {:?}", ports);

        // Create a shared governance that knows about all nodes
        let shared_governance = {
            let attestor = MockAttestor::new();
            let actual_pcrs = attestor.pcrs_sync();
            let test_version = Version {
                ne_pcr0: actual_pcrs.pcr0,
                ne_pcr1: actual_pcrs.pcr1,
                ne_pcr2: actual_pcrs.pcr2,
            };

            let governance = Arc::new(MockGovernance::new(
                vec![], // Start with empty topology
                vec![test_version],
                "http://localhost:3200".to_string(),
                vec![],
            ));

            // Add all nodes to the shared governance
            for (i, (&port, signing_key)) in ports.iter().zip(signing_keys.iter()).enumerate() {
                let node = GovernanceNode {
                    availability_zone: "test-az".to_string(),
                    origin: format!("http://127.0.0.1:{}", port),
                    public_key: signing_key.verifying_key(),
                    region: "test-region".to_string(),
                    specializations: HashSet::new(),
                };

                governance
                    .add_node(node)
                    .expect("Failed to add node to governance");
                println!("‚úÖ Added node {} to shared governance", i);
            }

            governance
        };

        // Create consensus nodes using the shared governance
        for (i, (&port, signing_key)) in ports.iter().zip(signing_keys.iter()).enumerate() {
            let attestor = Arc::new(MockAttestor::new());

            let config = ConsensusConfig {
                governance: shared_governance.clone(),
                attestor: attestor.clone(),
                signing_key: signing_key.clone(),
                raft_config: RaftConfig::default(),
                transport_config: proven_consensus::transport::TransportConfig::Tcp {
                    listen_addr: format!("127.0.0.1:{port}").parse().unwrap(),
                },
                storage_config: proven_consensus::config::StorageConfig::Memory,
                cluster_discovery_timeout: None,
                cluster_join_retry_config:
                    proven_consensus::config::ClusterJoinRetryConfig::default(),
            };

            let consensus = Consensus::new(config).await.unwrap();
            nodes.push(consensus);
            println!(
                "‚úÖ Created node {} listening on port {} with shared governance",
                i, port
            );
        }

        println!(
            "‚úÖ Created {} nodes (networks already started)",
            nodes.len()
        );

        // Wait a moment for listeners to be ready
        tokio::time::sleep(Duration::from_millis(200)).await;

        // Test cluster discovery from node 0 (this should use correlation IDs)
        println!("üîç Starting cluster discovery from node 0...");
        let discovery_result = nodes[0].discover_existing_clusters().await;

        println!("üìã Discovery result: {:?}", discovery_result);

        // Discovery should work even if nodes don't have active clusters yet
        assert!(discovery_result.is_ok(), "Discovery should succeed");
        let responses = discovery_result.unwrap();
        println!("‚úÖ Received {} discovery responses", responses.len());

        // Each node should respond to discovery requests
        assert_eq!(
            responses.len(),
            1,
            "Should receive 1 discovery response from the other node"
        );

        // Verify the response content
        let response = &responses[0];
        assert_eq!(
            response.responder_id,
            NodeId::new(signing_keys[1].verifying_key())
        );
        assert!(!response.has_active_cluster); // Nodes are not in clusters yet

        println!("‚úÖ Cluster discovery with correlation IDs test completed successfully");

        // Cleanup
        for node in nodes {
            let _ = node.shutdown().await;
        }
    }

    #[tokio::test]
    #[traced_test]
    async fn test_end_to_end_discovery_and_join_flow() {
        println!("üß™ Testing complete end-to-end discovery and join flow");

        let num_nodes = 3;
        let mut nodes = Vec::new();
        let mut ports = Vec::new();
        let mut signing_keys = Vec::new();

        // Allocate ports and generate keys for all nodes
        for i in 0..num_nodes {
            let port = proven_util::port_allocator::allocate_port();
            println!("üîå Allocated port {} for node {}", port, i);
            ports.push(port);
            signing_keys.push(SigningKey::generate(&mut OsRng));
        }

        println!("üìã All allocated ports: {:?}", ports);

        // Create a shared governance that knows about all nodes
        let shared_governance = {
            let attestor = MockAttestor::new();
            let actual_pcrs = attestor.pcrs_sync();
            let test_version = Version {
                ne_pcr0: actual_pcrs.pcr0,
                ne_pcr1: actual_pcrs.pcr1,
                ne_pcr2: actual_pcrs.pcr2,
            };

            let governance = Arc::new(MockGovernance::new(
                vec![], // Start with empty topology
                vec![test_version],
                "http://localhost:3200".to_string(),
                vec![],
            ));

            // Add all nodes to the shared governance
            for (i, (&port, signing_key)) in ports.iter().zip(signing_keys.iter()).enumerate() {
                let node = GovernanceNode {
                    availability_zone: "test-az".to_string(),
                    origin: format!("http://127.0.0.1:{}", port),
                    public_key: signing_key.verifying_key(),
                    region: "test-region".to_string(),
                    specializations: HashSet::new(),
                };

                governance
                    .add_node(node)
                    .expect("Failed to add node to governance");
                println!("‚úÖ Added node {} to shared governance", i);
            }

            governance
        };

        // Create consensus nodes using the shared governance
        for (i, (&port, signing_key)) in ports.iter().zip(signing_keys.iter()).enumerate() {
            let attestor = Arc::new(MockAttestor::new());

            let config = ConsensusConfig {
                governance: shared_governance.clone(),
                attestor: attestor.clone(),
                signing_key: signing_key.clone(),
                raft_config: RaftConfig::default(),
                transport_config: proven_consensus::transport::TransportConfig::Tcp {
                    listen_addr: format!("127.0.0.1:{port}").parse().unwrap(),
                },
                storage_config: proven_consensus::config::StorageConfig::Memory,
                cluster_discovery_timeout: None,
                cluster_join_retry_config:
                    proven_consensus::config::ClusterJoinRetryConfig::default(),
            };

            let consensus = Consensus::new(config).await.unwrap();
            nodes.push(consensus);
            println!("‚úÖ Created node {} on port {}", i, port);
        }

        println!("\nüé¨ Starting end-to-end test scenario");

        // Step 1: Start the first node as cluster leader
        println!("\nüìç Step 1: Starting node 0 as initial cluster leader");
        let leader_start_result = nodes[0].start().await;
        assert!(
            leader_start_result.is_ok(),
            "Leader node should start successfully"
        );

        // Wait for leader to be established
        tokio::time::sleep(Duration::from_secs(2)).await;

        // Verify node 0 is the leader
        let mut attempts = 0;
        while !nodes[0].is_leader() && attempts < 10 {
            tokio::time::sleep(Duration::from_millis(200)).await;
            attempts += 1;
        }
        assert!(nodes[0].is_leader(), "Node 0 should become the leader");
        println!("‚úÖ Node 0 is now the cluster leader");

        // Step 2: Test discovery from node 1
        println!("\nüìç Step 2: Testing cluster discovery from node 1");
        let discovery_results = nodes[1].discover_existing_clusters().await.unwrap();
        println!(
            "üîç Discovery found {} active clusters",
            discovery_results.len()
        );

        // Should find the leader's cluster
        assert!(
            !discovery_results.is_empty(),
            "Should discover at least one cluster"
        );

        let leader_cluster = discovery_results
            .iter()
            .find(|r| r.has_active_cluster)
            .expect("Should find an active cluster from the leader");

        assert!(
            leader_cluster.has_active_cluster,
            "Leader should report active cluster"
        );
        assert_eq!(
            leader_cluster.responder_id,
            NodeId::new(signing_keys[0].verifying_key()),
            "Should discover leader cluster from node 0"
        );
        println!("‚úÖ Successfully discovered leader's cluster");

        // Step 3: Test joining the cluster
        println!("\nüìç Step 3: Node 1 joining the cluster via NetworkManager");

        // Use GlobalManager's join method directly
        let join_result = nodes[1]
            .global_manager()
            .join_existing_cluster_via_raft(
                NodeId::new(signing_keys[1].verifying_key()),
                leader_cluster,
            )
            .await;

        assert!(
            join_result.is_ok(),
            "Node 1 should successfully join the cluster: {:?}",
            join_result
        );
        println!("‚úÖ Node 1 successfully joined the cluster");

        // Step 4: Verify cluster membership
        println!("\nüìç Step 4: Verifying cluster membership");

        // Wait for membership changes to propagate
        tokio::time::sleep(Duration::from_secs(3)).await;

        // Check cluster state on both nodes
        let leader_cluster_size = nodes[0].cluster_size();
        let joiner_cluster_size = nodes[1].cluster_size();

        println!("Leader reports cluster size: {:?}", leader_cluster_size);
        println!("Joiner reports cluster size: {:?}", joiner_cluster_size);

        // Both nodes should see the same cluster size
        assert_eq!(
            leader_cluster_size,
            Some(2),
            "Leader should see cluster size of 2"
        );
        assert_eq!(
            joiner_cluster_size,
            Some(2),
            "Joiner should see cluster size of 2"
        );

        // Check that both nodes agree on the leader
        let leader_id_from_leader = nodes[0].current_leader().await;
        let leader_id_from_joiner = nodes[1].current_leader().await;
        assert_eq!(
            leader_id_from_leader, leader_id_from_joiner,
            "Both nodes should agree on the leader"
        );
        println!("‚úÖ Both nodes agree on cluster membership");

        // Step 5: Test a third node joining
        println!("\nüìç Step 5: Testing third node joining");

        // Discover from node 2
        let discovery_results_2 = nodes[2].discover_existing_clusters().await.unwrap();
        assert!(
            !discovery_results_2.is_empty(),
            "Node 2 should discover the existing cluster"
        );

        let cluster_info = discovery_results_2
            .iter()
            .find(|r| r.has_active_cluster)
            .expect("Should find the existing cluster");

        // Join the cluster
        let join_result_2 = nodes[2]
            .global_manager()
            .join_existing_cluster_via_raft(
                NodeId::new(signing_keys[2].verifying_key()),
                cluster_info,
            )
            .await;

        assert!(
            join_result_2.is_ok(),
            "Node 2 should successfully join the cluster: {:?}",
            join_result_2
        );
        println!("‚úÖ Node 2 successfully joined the cluster");

        // Step 6: Final verification
        println!("\nüìç Step 6: Final cluster verification");

        // Wait for final membership changes
        tokio::time::sleep(Duration::from_secs(3)).await;

        // All nodes should see cluster size of 3
        for (i, node) in nodes.iter().enumerate() {
            let cluster_size = node.cluster_size();
            assert_eq!(
                cluster_size,
                Some(3),
                "Node {} should see cluster size of 3, got {:?}",
                i,
                cluster_size
            );
            println!("‚úÖ Node {} reports cluster size: {:?}", i, cluster_size);
        }

        // Test cluster functionality - submit a request through the leader
        println!("\nüìç Step 7: Testing cluster functionality");
        let test_request = GlobalRequest {
            operation: GlobalOperation::PublishToStream {
                stream: "test-cluster-stream".to_string(),
                data: Bytes::from("cluster is working!"),
                metadata: None,
            },
        };

        let submit_result = nodes[0].submit_request(test_request).await;
        assert!(
            submit_result.is_ok(),
            "Should be able to submit request to cluster"
        );
        println!("‚úÖ Successfully submitted request to cluster");

        // Verify the message was applied to the state machine
        tokio::time::sleep(Duration::from_millis(100)).await;
        let stream_store = nodes[0].stream_store();
        let last_seq = stream_store.last_sequence("test-cluster-stream").await;
        assert!(
            last_seq > 0,
            "Message should have been applied to state machine"
        );
        println!(
            "‚úÖ Message applied to state machine with sequence {}",
            last_seq
        );

        // Graceful shutdown
        println!("\nüõë Shutting down all nodes...");
        for (i, node) in nodes.iter().enumerate() {
            let shutdown_result = node.shutdown().await;
            assert!(
                shutdown_result.is_ok(),
                "Node {} should shutdown cleanly",
                i
            );
            println!("‚úÖ Node {} shutdown successfully", i);
        }

        println!("\nüéâ END-TO-END TEST COMPLETED SUCCESSFULLY!");
        println!("‚úÖ Discovery: Node 1 found leader's cluster");
        println!("‚úÖ Join: Node 1 successfully joined via cluster join request");
        println!("‚úÖ Membership: All nodes agree on cluster membership");
        println!("‚úÖ Third Join: Node 2 successfully joined the existing cluster");
        println!("‚úÖ Functionality: Cluster processes requests correctly");
        println!("‚úÖ Final State: 3-node cluster with proper consensus");
    }

    #[tokio::test]
    #[traced_test]
    async fn test_websocket_leader_election() {
        println!("üß™ Testing WebSocket-based leader election");

        let num_nodes = 3;
        let mut nodes = Vec::new();
        let mut ports = Vec::new();
        let mut signing_keys = Vec::new();
        let mut http_servers = Vec::new();

        // Allocate ports and generate keys for all nodes
        for _i in 0..num_nodes {
            ports.push(next_port());
            signing_keys.push(SigningKey::generate(&mut OsRng));
        }

        println!("üìã Allocated ports: {:?}", ports);

        // Create a shared governance that knows about all nodes
        let shared_governance = {
            let attestor = MockAttestor::new();
            let actual_pcrs = attestor.pcrs_sync();
            let test_version = Version {
                ne_pcr0: actual_pcrs.pcr0,
                ne_pcr1: actual_pcrs.pcr1,
                ne_pcr2: actual_pcrs.pcr2,
            };

            let governance = Arc::new(MockGovernance::new(
                vec![], // Start with empty topology
                vec![test_version],
                "http://localhost:3200".to_string(),
                vec![],
            ));

            // Add all nodes to the shared governance
            for (i, (&port, signing_key)) in ports.iter().zip(signing_keys.iter()).enumerate() {
                let node = GovernanceNode {
                    availability_zone: "test-az".to_string(),
                    origin: format!("http://127.0.0.1:{}", port),
                    public_key: signing_key.verifying_key(),
                    region: "test-region".to_string(),
                    specializations: HashSet::new(),
                };

                governance
                    .add_node(node)
                    .expect("Failed to add node to governance");
                println!("‚úÖ Added node {} to shared governance", i);
            }

            governance
        };

        // Create consensus nodes using the shared governance with WebSocket transport
        for signing_key in signing_keys.iter() {
            let attestor = Arc::new(MockAttestor::new());

            let config = ConsensusConfig {
                governance: shared_governance.clone(),
                attestor: attestor.clone(),
                signing_key: signing_key.clone(),
                raft_config: RaftConfig::default(),
                transport_config: proven_consensus::transport::TransportConfig::WebSocket,
                storage_config: proven_consensus::config::StorageConfig::Memory,
                cluster_discovery_timeout: None,
                cluster_join_retry_config:
                    proven_consensus::config::ClusterJoinRetryConfig::default(),
            };

            let consensus = Consensus::new(config).await.unwrap();
            nodes.push(consensus);
        }

        // Start HTTP servers for each node with WebSocket integration
        println!("üöÄ Starting HTTP servers with WebSocket integration...");
        for (i, (node, &port)) in nodes.iter().zip(ports.iter()).enumerate() {
            // Get the WebSocket router from the consensus
            let router = node.create_router().expect("Should create router");

            // Create the HTTP server
            let listener = tokio::net::TcpListener::bind(format!("127.0.0.1:{}", port))
                .await
                .expect("Should bind to port");

            println!("‚úÖ Node {} HTTP server listening on port {}", i, port);

            // Start the HTTP server in the background
            let server_handle = tokio::spawn(async move {
                axum::serve(listener, router)
                    .await
                    .expect("HTTP server should run");
            });

            http_servers.push(server_handle);

            // Short delay to let server start
            tokio::time::sleep(Duration::from_millis(100)).await;
        }

        println!("‚úÖ All HTTP servers started with WebSocket endpoints");

        // Start all consensus nodes
        println!("üöÄ Starting all consensus nodes...");
        for (i, node) in nodes.iter().enumerate() {
            println!(
                "Starting WebSocket consensus node {} ({})",
                i,
                &node.node_id()
            );
            let start_result = node.start().await;
            assert!(
                start_result.is_ok(),
                "WebSocket node {} start should succeed: {start_result:?}",
                i
            );
            // Short delay to allow listener to start
            tokio::time::sleep(Duration::from_millis(200)).await;
        }

        println!("‚úÖ All WebSocket consensus nodes started");

        // Give time for WebSocket connections and cluster formation
        println!("‚è≥ Waiting for WebSocket cluster formation and leader election...");
        tokio::time::sleep(Duration::from_secs(10)).await;

        // Collect cluster state from all nodes
        let mut leader_count = 0;
        let mut cluster_leaders = Vec::new();
        let mut cluster_sizes = Vec::new();
        let mut cluster_terms = Vec::new();

        println!("üìä Analyzing WebSocket cluster state:");
        for (i, node) in nodes.iter().enumerate() {
            let is_leader = node.is_leader();
            let current_leader = node.current_leader().await;
            let cluster_size = node.cluster_size();
            let current_term = node.current_term();

            println!(
                "   WebSocket Node {} - Leader: {}, Current Leader: {:?}, Term: {:?}, Cluster Size: {:?}",
                i,
                is_leader,
                current_leader.as_ref().map(|id| &id[..8]),
                current_term,
                cluster_size
            );

            if is_leader {
                leader_count += 1;
            }

            cluster_leaders.push(current_leader);
            cluster_sizes.push(cluster_size);
            cluster_terms.push(current_term);
        }

        // Assert correct WebSocket cluster formation
        println!("üîç Verifying correct WebSocket cluster formation...");

        // 1. There should be exactly 1 leader across all nodes
        assert_eq!(
            leader_count, 1,
            "Expected exactly 1 leader in WebSocket cluster, found {}",
            leader_count
        );
        println!("‚úÖ Exactly 1 leader found in WebSocket cluster");

        // 2. All nodes should report the same cluster size of 3
        let expected_cluster_size = Some(num_nodes);
        for (i, &cluster_size) in cluster_sizes.iter().enumerate() {
            assert_eq!(
                cluster_size, expected_cluster_size,
                "WebSocket Node {} reports cluster size {:?}, expected {:?}",
                i, cluster_size, expected_cluster_size
            );
        }
        println!(
            "‚úÖ All WebSocket nodes report cluster size of {}",
            num_nodes
        );

        // 3. All nodes should agree on who the leader is
        let first_leader = &cluster_leaders[0];
        assert!(
            first_leader.is_some(),
            "No leader found in WebSocket cluster"
        );
        for (i, leader) in cluster_leaders.iter().enumerate() {
            assert_eq!(
                leader,
                first_leader,
                "WebSocket Node {} reports leader {:?}, expected {:?}",
                i,
                leader.as_ref().map(|s| &s[..8]),
                first_leader.as_ref().map(|s| &s[..8])
            );
        }
        println!(
            "‚úÖ All WebSocket nodes agree on leader: {}",
            &first_leader.as_ref().unwrap()[..8]
        );

        // 4. All nodes should have the same term (indicating same cluster)
        let first_term = cluster_terms[0];
        assert!(
            first_term.is_some() && first_term.unwrap() > 0,
            "Invalid term in WebSocket cluster"
        );
        for (i, &term) in cluster_terms.iter().enumerate() {
            assert_eq!(
                term, first_term,
                "WebSocket Node {} reports term {:?}, expected {:?}",
                i, term, first_term
            );
        }
        println!(
            "‚úÖ All WebSocket nodes agree on term: {}",
            first_term.unwrap()
        );

        println!(
            "üéâ SUCCESS: WebSocket cluster formation with {} members and 1 leader!",
            num_nodes
        );

        // Test WebSocket cluster functionality - only the leader should accept writes
        println!("üîç Testing WebSocket cluster functionality...");
        let leader_node = nodes
            .iter()
            .find(|node| node.is_leader())
            .expect("Should have exactly one leader");

        let request = GlobalRequest {
            operation: GlobalOperation::PublishToStream {
                stream: "websocket-cluster-test".to_string(),
                data: Bytes::from("test message from WebSocket cluster"),
                metadata: None,
            },
        };

        match leader_node.submit_request(request).await {
            Ok(_) => {
                println!("‚úÖ WebSocket leader successfully processed operation");
            }
            Err(e) => {
                panic!("WebSocket leader failed to process operation: {}", e);
            }
        }

        // Verify the message was applied to the state machine
        tokio::time::sleep(Duration::from_millis(100)).await;
        let stream_store = leader_node.stream_store();
        let last_seq = stream_store.last_sequence("websocket-cluster-test").await;
        assert!(
            last_seq > 0,
            "Message should have been applied to WebSocket cluster state machine"
        );
        println!(
            "‚úÖ WebSocket cluster message applied with sequence {}",
            last_seq
        );

        println!("‚úÖ WebSocket cluster functionality verified");

        // Test WebSocket connection status
        println!("üîç Testing WebSocket connection status...");
        for (i, node) in nodes.iter().enumerate() {
            match node.get_connected_peers().await {
                Ok(peers) => {
                    println!(
                        "‚úÖ WebSocket Node {} has {} connected peers",
                        i,
                        peers.len()
                    );
                    for (peer_id, connected, _) in peers {
                        println!(
                            "  - Peer {}: connected={}",
                            &peer_id.to_hex()[..8],
                            connected
                        );
                    }
                }
                Err(e) => {
                    println!("‚ö†Ô∏è  WebSocket Node {} failed to get peer info: {}", i, e);
                }
            }
        }

        // Graceful shutdown
        println!("üõë Shutting down WebSocket cluster...");
        for (i, node) in nodes.iter().enumerate() {
            let shutdown_result = node.shutdown().await;
            assert!(
                shutdown_result.is_ok(),
                "WebSocket Node {} shutdown should succeed",
                i
            );
            println!("‚úÖ WebSocket Node {} shutdown successfully", i);
        }

        // Shutdown HTTP servers
        println!("üõë Shutting down HTTP servers...");
        for (i, server_handle) in http_servers.into_iter().enumerate() {
            server_handle.abort();
            println!("‚úÖ HTTP server {} shutdown", i);
        }

        println!("üéØ WebSocket leader election test completed successfully!");
        println!(
            "üéâ Verified: WebSocket cluster with {} members and 1 leader using HTTP integration",
            num_nodes
        );
    }
}
